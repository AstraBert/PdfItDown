{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035fccd4",
   "metadata": {
    "id": "035fccd4"
   },
   "source": [
    "# Customizing PdfItDown\n",
    "\n",
    "Once your app reaches a certain complexity, it might be good to customize it so that it can perform more complex and document-specific task that better adapt to your use case.\n",
    "\n",
    "PdfItDown is designed to offer this flexibility, by allowing you to provide a custom `conversion_callback` to the `Converter` class constructor.\n",
    "\n",
    "The `conversion_callback`, tho, has to follow this function signature:\n",
    "\n",
    "```python\n",
    "def conversion_callback(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    title: str | None,\n",
    "    overwrite: bool\n",
    ")\n",
    "```\n",
    "\n",
    "While the name of the parameters can vary, their order should not be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b0ab9",
   "metadata": {
    "id": "428b0ab9"
   },
   "source": [
    "## Example 1: Turn Word Documents into FAQs sheets\n",
    "\n",
    "The use case for this example is simple: a lot of times, teams drown in Google Docs and don't know where to start. With PdfItDown, you can turn `.docx` documents into PDFs, but in this case we will use some custom logic to extract an abstract and some FAQs and save everything into our PDF. For this, we will be using [LlamaExtract]() by LlamaIndex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb32026",
   "metadata": {
    "id": "ceb32026"
   },
   "outputs": [],
   "source": [
    "# Install needed dependencies\n",
    "! pip install -q \"pdfitdown>=2.0.0b2\" llama-cloud-services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "PiIe-G7IV-T2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiIe-G7IV-T2",
    "outputId": "33d6f6c6-a166-47c6-9c8b-7c4063303fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaCloud API Key: ··········\n"
     ]
    }
   ],
   "source": [
    "# Let's set up the API key for LlamaCloud\n",
    "# Get yours at https://cloud.llamaindex.ai !\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass(\"LlamaCloud API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c94ad",
   "metadata": {
    "id": "3c4c94ad"
   },
   "outputs": [],
   "source": [
    "# let's define the utilities for the FAQs and abstract extraction\n",
    "\n",
    "from llama_cloud_services.extract import LlamaExtract, ExtractConfig, SourceText\n",
    "from llama_cloud.types.extract_run import ExtractRun\n",
    "from llama_cloud.types.extract_mode import ExtractMode\n",
    "from llama_cloud_services.parse.utils import ResultType\n",
    "from llama_cloud_services import LlamaParse\n",
    "from typing import cast\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class QuestionAndAnswer(BaseModel):\n",
    "    question: str = Field(description=\"Frequently asked question\")\n",
    "    answer: str = Field(description=\"Answer to the question\")\n",
    "\n",
    "\n",
    "class DocumentBrief(BaseModel):\n",
    "    abstract: str = Field(description=\"Abstract of the document\")\n",
    "    faqs: list[QuestionAndAnswer] = Field(description=\"FAQs about the document\")\n",
    "\n",
    "\n",
    "extractor = LlamaExtract(api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"))\n",
    "extract_config = ExtractConfig(\n",
    "    extraction_mode=ExtractMode.MULTIMODAL,\n",
    ")\n",
    "parser = LlamaParse(api_key=os.getenv(\"LLAMA_CLOUD_API_KEY\"), result_type=ResultType.MD)\n",
    "\n",
    "\n",
    "def extract_faqs_and_abstract(input_path: str):\n",
    "    result = parser.parse(file_path=input_path)\n",
    "    docs = result.get_markdown_documents()\n",
    "    text = \"\\n\".join([doc.text for doc in docs])\n",
    "    extract_result = extractor.extract(\n",
    "        data_schema=DocumentBrief,\n",
    "        config=extract_config,\n",
    "        files=SourceText(text_content=text),\n",
    "    )\n",
    "    data = cast(ExtractRun, extract_result).data\n",
    "    return DocumentBrief.model_validate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v34rUZNaXBuJ",
   "metadata": {
    "id": "v34rUZNaXBuJ"
   },
   "outputs": [],
   "source": [
    "# Now let's create the custom PDF converter\n",
    "from markdown_pdf import MarkdownPdf, Section\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def conversion_callback(\n",
    "    input_path: str, output_path: str, title: str | None = None, overwrite: bool = True\n",
    ") -> str:\n",
    "    if Path(output_path).exists() and not overwrite:\n",
    "        raise FileExistsError(f\"File {output_path} already exists\")\n",
    "    document_brief_data = extract_faqs_and_abstract(input_path=input_path)\n",
    "    abstract = document_brief_data.abstract\n",
    "    faqs = document_brief_data.faqs\n",
    "    content = f\"# Brief for {input_path}\\n\\n## Abstract\\n\\n{abstract}\\n\\n## FAQs\\n\\n\"\n",
    "    for faq in faqs:\n",
    "        content += f\"### {faq.question}\\n\\n{faq.answer}\\n\\n\"\n",
    "    pdf = MarkdownPdf(toc_level=0)\n",
    "    pdf.add_section(Section(content))\n",
    "    pdf.meta[\"title\"] = title or f\"Brief for {input_path}\"\n",
    "    pdf.save(output_path)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "FAq_EZKEYdVR",
   "metadata": {
    "id": "FAq_EZKEYdVR"
   },
   "outputs": [],
   "source": [
    "# Now let's convert!\n",
    "from pdfitdown.pdfconversion import Converter\n",
    "\n",
    "converter = Converter(conversion_callback=conversion_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "xEoUXs74ZkCU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "xEoUXs74ZkCU",
    "outputId": "914097cd-2920-4bdf-f254-ed73c72898a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id fa2cfe74-0a56-4b04-ac22-2336b1334be8\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'strategic_plan_brief.pdf'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.convert(\n",
    "    file_path=\"data/strategic_plan.docx\",\n",
    "    output_path=\"strategic_plan_brief.pdf\",\n",
    "    overwrite=True,\n",
    "    title=\"Strategic Plan Brief\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bQH8B8ZVadiJ",
   "metadata": {
    "id": "bQH8B8ZVadiJ"
   },
   "source": [
    "## Example 2: Generate PDF documents from LLM responses\n",
    "\n",
    "LLMs produce their responses as markdown text. We can exploit this to create PDF documents for their responses, to have, for instance, a chatbot that produces accurate news reports in PDF format.\n",
    "\n",
    "We will be using [LinkUp]() for fetching news and [OpenAI Structured Output]() to produce the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VjpfmkvgfJMh",
   "metadata": {
    "id": "VjpfmkvgfJMh"
   },
   "outputs": [],
   "source": [
    "# install needed dependencies\n",
    "\n",
    "! pip install linkup-sdk openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "EKHO2YssfiVT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKHO2YssfiVT",
    "outputId": "6ac3a83e-5bce-4185-c3f6-ce93ba17f68b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key: ··········\n",
      "Linkup API key: ··········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key: \")\n",
    "os.environ[\"LINKUP_API_KEY\"] = getpass(\"Linkup API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8jR6jKrPfPrm",
   "metadata": {
    "id": "8jR6jKrPfPrm"
   },
   "outputs": [],
   "source": [
    "from linkup import LinkupClient, LinkupSourcedAnswer\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "linkup_client = LinkupClient(api_key=os.getenv(\"LINKUP_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "rPJ-F_E_hMUC",
   "metadata": {
    "id": "rPJ-F_E_hMUC"
   },
   "outputs": [],
   "source": [
    "# let's define our news search utilities\n",
    "\n",
    "\n",
    "def search_news(query: str) -> LinkupSourcedAnswer:\n",
    "    response = linkup_client.search(\n",
    "        query=query,\n",
    "        depth=\"standard\",\n",
    "        output_type=\"sourcedAnswer\",\n",
    "        include_images=False,\n",
    "        include_inline_citations=False,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "Xfqc1DB3gIak",
   "metadata": {
    "id": "Xfqc1DB3gIak"
   },
   "outputs": [],
   "source": [
    "# let's create the report generation functions\n",
    "\n",
    "\n",
    "class NewsReport(BaseModel):\n",
    "    title: str = Field(description=\"Title of the report\")\n",
    "    report: str = Field(description=\"News report on a given topic\")\n",
    "    sources: list[str] = Field(description=\"Sources for the report\")\n",
    "\n",
    "\n",
    "def llm_generate(prompt: str, information: str) -> NewsReport | None:\n",
    "    result = openai_client.responses.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a reporter who is tasked, given the provided information and user prompt, to produce a comprehensive report, assigning it a title and listing the sources.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Initial user prompt:\\n\\n'''\\n{prompt}\\n'''\\n\\nCollected information from the web:\\n\\n'''\\n{information}\\n'''\\n\\n\",\n",
    "            },\n",
    "        ],\n",
    "        text_format=NewsReport,\n",
    "    )\n",
    "    return result.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "IljO8hN3hqTt",
   "metadata": {
    "id": "IljO8hN3hqTt"
   },
   "outputs": [],
   "source": [
    "# let's define the conversion callback, using a little trick: we take the first\n",
    "# input parameter as the news query (not a file path)!\n",
    "\n",
    "\n",
    "def conversion_callback(\n",
    "    query: str, output_path: str, title: str | None = None, overwrite: bool = True\n",
    ") -> str:\n",
    "    if Path(output_path).exists() and not overwrite:\n",
    "        raise FileExistsError(f\"File {output_path} already exists\")\n",
    "    response = search_news(query=query)\n",
    "    information = response.answer\n",
    "    complete_info = (\n",
    "        \"# Answer\\n\\n\"\n",
    "        + information\n",
    "        + \"# Sources\\n\\n\"\n",
    "        + \"\\n\".join(\n",
    "            [f\"**{source.url}**\\n\\n{source.snippet}\\n\\n\" for source in response.sources]\n",
    "        )\n",
    "    )\n",
    "    report = llm_generate(prompt=query, information=complete_info)\n",
    "    if report is None:\n",
    "        raise ValueError(\"LLM returned None\")\n",
    "    content = f\"# {report.title}\\n\\n{report.report}\\n\\n## Sources\\n\\n{'\\n'.join(report.sources)}\"\n",
    "    pdf = MarkdownPdf(toc_level=0)\n",
    "    pdf.add_section(Section(content))\n",
    "    pdf.meta[\"title\"] = title or report.title\n",
    "    pdf.save(output_path)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "wJ1ewuT1iwKt",
   "metadata": {
    "id": "wJ1ewuT1iwKt"
   },
   "outputs": [],
   "source": [
    "# Now let's convert!\n",
    "converter = Converter(conversion_callback=conversion_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "D5LwChKli0If",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "id": "D5LwChKli0If",
    "outputId": "da97db2e-3379-47b0-c8c3-b458eb13c8ed"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'eu_news_report.pdf'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.convert(\n",
    "    file_path=\"Most recent breaking news about the EU\",\n",
    "    output_path=\"eu_news_report.pdf\",\n",
    "    overwrite=True,\n",
    "    title=\"EU Breaking News\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
